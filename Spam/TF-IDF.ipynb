{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21807e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importació de llibreries essencials per al processament de text, aprenentatge automàtic i manipulació de dades\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8dec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descàrrega de recursos de NLTK per a la tokenització, lematització i etiquetatge gramatical\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0553d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []    }
   ],
   "source": [
    "# Funció per eliminar emojis mitjançant una expressió regular que cobreix la majoria de rangs Unicode corresponents\n",
    "def eliminar_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        patron = re.compile(\"[\" \n",
    "            \"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\"\n",
    "            \"\\U0001F680-\\U0001F6FF\" \"\\U0001F700-\\U0001F77F\"\n",
    "            \"\\U0001F780-\\U0001F7FF\" \"\\U0001F800-\\U0001F8FF\"\n",
    "            \"\\U0001F900-\\U0001F9FF\" \"\\U0001FA00-\\U0001FA6F\"\n",
    "            \"\\U0001FA70-\\U0001FAFF\" \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\" \"]+\", flags=re.UNICODE)\n",
    "        return patron.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Inicialització del lematitzador basat en WordNet\n",
    "lematitzador = WordNetLemmatizer()\n",
    "\n",
    "# Funció auxiliar per obtenir l'etiqueta gramatical (POS) d'una paraula, necessària per lematitzar correctament\n",
    "def obtenir_pos_tag(paraula):\n",
    "    etiqueta = nltk.pos_tag([paraula])[0][1][0].upper()\n",
    "    tipus = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tipus.get(etiqueta, wordnet.NOUN)  # Per defecte, es considera un nom (noun)\n",
    "\n",
    "# Funció principal de lematització: tokenitza el text i aplica la lematització amb POS\n",
    "def lematitzar_text(text):\n",
    "    paraules = nltk.word_tokenize(text)\n",
    "    return ' '.join([lematitzador.lemmatize(p, obtenir_pos_tag(p)) for p in paraules])\n",
    "\n",
    "\n",
    "# Carreguem el conjunt de dades SMS etiquetat (ham/spam) des d’una font pública\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['etiqueta', 'missatge'])\n",
    "\n",
    "# Conversió de les etiquetes textuals a valors binaris: 'ham' → 0 i 'spam' → 1\n",
    "df['etiqueta'] = df['etiqueta'].replace({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Aplicació del preprocessament: eliminació d’emojis i lematització dels missatges\n",
    "df['missatge'] = df['missatge'].apply(eliminar_emojis).apply(lematitzar_text)\n",
    "\n",
    "# Definició de stopwords (paraules buides) que seran excloses del model de TF-IDF\n",
    "stopwords = ['a', 'an', 'the', 'in', 'on', 'at', 'to', 'of', 'and', 'or',\n",
    "            'is', 'it', 'for', 'with', 'that', 'this', 'as', 'was', 'be',\n",
    "            'are', 'were', 'been', 'from', 'by', 'about', 'into', 'out',\n",
    "            'up', 'down', 'over', 'under', 'then', 'than', 'so', 'but', 'not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbc8b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracio: 0\n",
      "AUC-ROC: Fold 0:0.9806632124352331\n",
      "Iteracio: 1\n",
      "AUC-ROC: Fold 1:0.9831778929188255\n",
      "Iteracio: 2\n",
      "AUC-ROC: Fold 2:0.9831206314984179\n",
      "Iteracio: 3\n",
      "AUC-ROC: Fold 3:0.9789825086066001\n",
      "Iteracio: 4\n",
      "AUC-ROC: Fold 4:0.9868484195152484\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "# Assignació de variables predictives (X) i de la variable objectiu (y)\n",
    "X_text = df['missatge']\n",
    "y = df['etiqueta']\n",
    "\n",
    "# Configuració de validació creuada estratificada amb 5 particions,\n",
    "# mantenint la proporció de classes en cada partició\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "# Bucle principal de validació creuada\n",
    "for train_idx, test_idx in skf.split(X_text, y):\n",
    "    # Separació del conjunt d'entrenament i de prova\n",
    "    X_train_text = X_text.iloc[train_idx]\n",
    "    X_test_text = X_text.iloc[test_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "\n",
    "    # Vectorització dels textos utilitzant TF-IDF amb unigramas i bigramas\n",
    "    # i un límit de 10.000 característiques. L’ajust es fa només amb el conjunt d’entrenament.\n",
    "    vectoritzador = TfidfVectorizer(max_features=10000, stop_words=stopwords)\n",
    "    X_train = vectoritzador.fit_transform(X_train_text)\n",
    "    X_test = vectoritzador.transform(X_test_text)\n",
    "\n",
    "    # Inicialització i entrenament del model XGBoost amb hiperparàmetres definits\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=500,        # Nombre d’arbres del model\n",
    "        max_depth=6,             # Profunditat màxima de cada arbre\n",
    "        learning_rate=0.01,      # Taxa d’aprenentatge\n",
    "        random_state=42          # Semilla per a resultats reproductibles\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Iteracio: \"+str(i) )  # Impressió per indicar el progrés de la iteració\n",
    "\n",
    "\n",
    "    # Predicció de probabilitats per a la classe positiva (spam) i càlcul de l'AUC-ROC\n",
    "    probabilitats = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, probabilitats)\n",
    "    scores.append(auc)\n",
    "    print(\"AUC-ROC: Fold \" + str(i) +  \" : \" + str(auc))  # Impressió de la puntuació AUC del fold actual\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7aa90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Resultats de validació creuada (mitjana AUC-ROC per a k=10000):\n",
      "\n",
      " k_components  auc_roc_mitjana\n",
      "        10000           0.9826\n"
     ]
    }
   ],
   "source": [
    "# Organitzem els resultats\n",
    "resultats=[]\n",
    "resultats.append({\n",
    "        'k_components': 10000,\n",
    "        'auc_roc_mitjana': np.mean(scores)\n",
    "    })\n",
    "\n",
    "df_resultats = pd.DataFrame(resultats)\n",
    "\n",
    "# Mostrar la taula de resultats\n",
    "print(\"\\n Resultats de validació creuada (mitjana AUC-ROC per a k=10000):\\n\")\n",
    "print(df_resultats.to_string(index=False, float_format=\"%.4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
