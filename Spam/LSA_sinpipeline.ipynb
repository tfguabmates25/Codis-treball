{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1efd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tonie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tonie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tonie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "C:\\Users\\tonie\\AppData\\Local\\Temp\\ipykernel_7036\\3754130502.py:59: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['etiqueta'] = df['etiqueta'].replace({'ham': 0, 'spam': 1})\n"
     ]
    }
   ],
   "source": [
    "# Importació de llibreries necessàries per a processament de text, \n",
    "# reducció de dimensionalitat, modelatge i avaluació\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Descàrrega de recursos de NLTK per a la tokenització i lematització\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Funcions auxiliars per al preprocessament de text\n",
    "\n",
    "# Eliminació d’emojis del text utilitzant una expressió regular que cobreix els rangs Unicode d’emoticones\n",
    "def eliminar_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        patron = re.compile(\"[\" \n",
    "            \"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\"\n",
    "            \"\\U0001F680-\\U0001F6FF\" \"\\U0001F700-\\U0001F77F\"\n",
    "            \"\\U0001F780-\\U0001F7FF\" \"\\U0001F800-\\U0001F8FF\"\n",
    "            \"\\U0001F900-\\U0001F9FF\" \"\\U0001FA00-\\U0001FA6F\"\n",
    "            \"\\U0001FA70-\\U0001FAFF\" \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\" \"]+\", flags=re.UNICODE)\n",
    "        return patron.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Inicialització del lematitzador de WordNet\n",
    "lematitzador = WordNetLemmatizer()\n",
    "\n",
    "# Obté el tipus gramatical (POS) d'una paraula, necessari per a una lematització precisa\n",
    "def obtenir_pos_tag(paraula):\n",
    "    etiqueta = nltk.pos_tag([paraula])[0][1][0].upper()\n",
    "    tipus = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tipus.get(etiqueta, wordnet.NOUN)\n",
    "\n",
    "# Funció de lematització completa d’un text (tokenització + lematització)\n",
    "def lematitzar_text(text):\n",
    "    paraules = nltk.word_tokenize(text)\n",
    "    paraules_lem = [lematitzador.lemmatize(p, obtenir_pos_tag(p)) for p in paraules]\n",
    "    return ' '.join(paraules_lem)\n",
    "\n",
    "# Càrrega i preprocessament del conjunt de dades\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['etiqueta', 'missatge'])\n",
    "\n",
    "# Conversió de les etiquetes textuals a valors binaris: 'ham' → 0, 'spam' → 1\n",
    "df['etiqueta'] = df['etiqueta'].replace({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Aplicació del netejat de text: eliminació d’emojis i lematització\n",
    "df['missatge'] = df['missatge'].apply(eliminar_emojis).apply(lematitzar_text)\n",
    "\n",
    "# Definició de paraules buides\n",
    "\n",
    "stopwords = ['a', 'an', 'the', 'in', 'on', 'at', 'to', 'of', 'and', 'or',\n",
    "            'is', 'it', 'for', 'with', 'that', 'this', 'as', 'was', 'be',\n",
    "            'are', 'were', 'been', 'from', 'by', 'about', 'into', 'out',\n",
    "            'up', 'down', 'over', 'under', 'then', 'than', 'so', 'but', 'not']\n",
    "\n",
    "# Definició de variables d’entrada i sortida\n",
    "\n",
    "X = df[\"missatge\"]\n",
    "y = df[\"etiqueta\"]\n",
    "\n",
    "# Definició de la llista de components (k) per a reducció SVD\n",
    "\n",
    "valors_k = [100]  # nombre de components a provar per SVD\n",
    "resultats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5140e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteracio\n",
      "iteracio\n",
      "iteracio\n",
      "iteracio\n",
      "iteracio\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Configuració de la validació creuada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Bucle principal: per cada valor de k, entrenar i avaluar el model\n",
    "for k in valors_k:\n",
    "    puntuacions = []\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        # Divisió del text en entrenament i test\n",
    "        X_train_raw, X_test_raw = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Vectorització TF-IDF ajustada exclusivament sobre l'entrenament\n",
    "        tfidf = TfidfVectorizer(max_features=10000, stop_words=stopwords)\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train_raw)\n",
    "        X_test_tfidf = tfidf.transform(X_test_raw)\n",
    "        # Reducció dimensional SVD ajustada només amb l'entrenament\n",
    "        svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "        X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "        X_test_svd = svd.transform(X_test_tfidf)\n",
    "        # Entrenament del model XGBoost\n",
    "        model = XGBClassifier(\n",
    "            max_depth=6,\n",
    "            n_estimators=500,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_svd, y_train)\n",
    "        print(\"iteracio\")\n",
    "\n",
    "        # Càlcul del AUC-ROC sobre el conjunt de test\n",
    "        probs = model.predict_proba(X_test_svd)[:, 1]\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "        puntuacions.append(auc)\n",
    "\n",
    "    # Emmagatzemar el resultat mitjà per a aquest valor de k\n",
    "    resultats.append({\n",
    "        'k_components': k,\n",
    "        'auc_roc_mitjana': np.mean(puntuacions)\n",
    "    })\n",
    "\n",
    "# Emmagatzamem els resultats\n",
    "df_resultats = pd.DataFrame(resultats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfe35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.9884628670120899), np.float64(0.9848911917098445), np.float64(0.9898042215808326), np.float64(0.9752581980039643), np.float64(0.9909726327502869)]\n",
      "   k_components  auc_roc_mitjana\n",
      "0           100         0.985878\n"
     ]
    }
   ],
   "source": [
    "#  Visualització de les puntuacions AUC obtingudes en les diferents iteracions de validació creuada i veiem resultats\n",
    "\n",
    "print(puntuacions)\n",
    "print(df_resultats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b87bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
